# -*- coding: utf-8 -*-
"""Proj.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AqqXR80j_rAdMcP3k8J76IYxXme5sXgN
"""

!pip install scrapy

import scrapy
import requests
import pandas as pd
import time
import numpy as np
from scrapy.http import TextResponse
from scrapy.crawler import CrawlerProcess
import geopy.distance
from sklearn.linear_model import LinearRegression
import statsmodels.formula.api as smf
import matplotlib as plt

# Commented out IPython magic to ensure Python compatibility.
# Ploting
# %matplotlib inline
# Controlling sizes of plots
# %pylab inline
# Change the size of plots
pylab.rcParams['figure.figsize'] = (15,5)
plt.style.use('ggplot')

"""## **Data Scraping Part**"""

URL = "https://www.tripadvisor.com/Restaurants-g293932-Yerevan.html"
base_url = "https://www.tripadvisor.com"

class Pages:

    def __init__(self,URL):
        self.URL = URL
        self.page = requests.get(self.URL)
        self.response = TextResponse(body=self.page.text,url=self.URL,encoding="utf-8")

    def url_scraper(self):
        """
       Getting urls for all top restaurants in Yerevan
        """
        
        url = [base_url + i for i in self.response.css('div.wQjYiB7z>span>a._15_ydu6b::attr(href)').extract()]
        return url
      
    def get_next(self):
        """
        If a NEXT button exists,get the next page's URL 
        """
        next_url = self.response.css("div[class='unified pagination js_pageLinks'] a[class='nav next rndBtn ui_button primary taLnk']::attr(href)").extract()
        return next_url

urls = []
pg = Pages(URL)

while True:
    time.sleep(1)
    if (pg.get_next()==[]):
        url = pg.url_scraper()
        urls.extend(url)
        break
    else:
        url = pg.url_scraper()
        urls.extend(url)
        url = base_url + pg.get_next()[0]
        pg = Pages(url)

df = pd.DataFrame({"url":urls})
df

class RestaurantsScraper(scrapy.Spider):
  name = "restaurants_scraper"
  start_urls = df.url.tolist()
  allowed_domains = ["https://www.tripadvisor.com/Restaurants-g293932-Yerevan.html"]

  custom_settings = {
      "FEED_FORMAT":"json",
      "FEED_URI":"restaurants.json",
      "ROBOTSTXT_OBEY":True,
      'DOWNLOAD_DELAY':0.5,
      'RANDOMIZE_DOWNLOAD_DELAY':True,
      
     
  }

  def parse(self,response):
    """
    As each digit in the numbers of reviews was presented as a separate element in a list we use the following join code to merge them together in order to make them integers.
    """
    rating = float(response.css("div[class='Ct2OcWS4 _3nlSp84z'] >span[class='r2Cf69qf']::text").extract_first(default=None))
    names = response.css("div[class='_1hkogt_o']>h1[class='_3a1XQ88S']::text").extract_first(default = None)
    reviews = response.css("div[class='Ct2OcWS4 _3nlSp84z'] >a[class='_10Iv7dOs']::text").re("\d*.\d+")[0]
    numbers= [item for item in reviews if item != ","]
    review = int([''.join(numbers[0:])][0]) 
    prices = response.css("div[class='_3acGlZjD'] div[class='_3UjHBXYa']>div:nth-child(1)>div[class='_14zKtJkz']~div[class='_1XLfiSsv']::text").extract_first()
    specdiets = response.css("div[class='_3acGlZjD'] div[class='_3UjHBXYa']>div:nth-child(3)>div[class='_14zKtJkz']~div[class='_1XLfiSsv']::text").extract_first()
    cuisines = response.css("div[class='_3acGlZjD'] div[class='_3UjHBXYa']>div:nth-child(2)>div[class='_14zKtJkz']~div[class='_1XLfiSsv']::text").extract_first()
    address = response.css("span[class='_13OzAOXO _2VxaSjVD']>span>a[class='_15QfMZ2L']::text").extract_first()
    yield {"names":names,"rating":rating,"reviews": review,"address":address,"cuisine":cuisines,"specdiet":specdiets,"price":prices,"url":response.url}

process = CrawlerProcess()
process.crawl(RestaurantsScraper)
process.start()

data = pd.read_json("restaurants.json")
data

# As some restaurants don't have prices section mentioned, the cuisine type occured under the same code as the price section. So we have deleted the restaurants with no price section mentioned. 
Data = data[~data['price'].str.contains("[a-zA-Z]").fillna(False)]
Data

#Remove restaurants with no price range mentioned under the price section.
Data['price'].replace('', np.nan, inplace=True)
Data.dropna(subset=['price'], inplace=True)
Data

#Removing duplicates(some restaurants appear many times in different pages)
df2 = Data.drop_duplicates(subset='names', keep='first')
df2

def dist_scraper(address):
  """
  Getting the distance of a particular restaurant from the Republic Square.
  """
  page = requests.get(f"https://geocode-maps.yandex.ru/1.x/?apikey=b22f08ae-80b7-4174-821c-604895f72a9f&format=json&geocode={address}&lang=en-US")
  response = page.json()
  cord = response['response']['GeoObjectCollection']['featureMember']
  if len(cord)>0:
    lat = float(cord[0]['GeoObject']['Point']['pos'].split(" ")[0])
    long = float(cord[0]['GeoObject']['Point']['pos'].split(" ")[1])
    coords_1 = (lat, long)
    coords_2 = (44.514711, 40.182982) #coordinates for Arami Str.
    dd = geopy.distance.vincenty(coords_1, coords_2).km
  else:
    lat = ""
    long = ""
    dd = ""
  return lat, long, dd

longtitude = []
latitude = []
distan = []
for i in df2.address.to_list():
  data = dist_scraper(address = i)
  latitude.append(data[0])
  longtitude.append(data[1])
  distan.append(data[2])

#Adding the distance to our dataframe
mydata = pd.DataFrame({"names":df2.names.to_list(),"rating":df2.rating.to_list(),"review":df2.reviews.to_list(),"cuisine":df2.cuisine.to_list(),"specdiet":df2.specdiet.to_list(),"price":df2.price.to_list(),"url":df2.url.to_list(),"address":df2.address.to_list(),"latitude":latitude,"longtitude":longtitude,"distance":distan})

# Removing the restaurants with no geolocation founded.
mydata['distance'].replace('', np.nan, inplace=True)
mydata.dropna(subset=['distance'], inplace=True)

min_price = []
max_price = []

# Getting maximum and the minimum prices from the price range.

for i in mydata['price']:
  data = i.split(" - ")
  min_price.append(int(data[0].replace("$","")))
  max_price.append(int(data[1].replace("$","").replace(",","")))

# Adding max and min prices instead of price range(prices are all in $)
mydata = pd.DataFrame({"names":mydata.names.to_list(),"rating":mydata.rating.to_list(),"review":mydata.review.to_list(),"cuisine":mydata.cuisine.to_list(),"specdiet":mydata.specdiet.to_list(),"min_price":min_price,"max_price":max_price,"url":mydata.url.to_list(),"address":mydata.address.to_list(),"latitude":mydata.latitude.to_list(),"longtitude":mydata.longtitude.to_list(),"distance":mydata.distance.to_list()})

# Droping rows with no special diets given 
mydata['specdiet'].replace('', np.nan, inplace=True)
mydata.dropna(subset=['specdiet'], inplace=True)

def types(cuisine):
  """
  Dividing cuisine type into Armenian and Not Armenian.
  """
  
  if "Armenian" in cuisine:
    type_ = "Armenian"
      
  else:
    type_ = "NotArmenian"
  return type_

cuisines = []

for i in mydata.cuisine.to_list():
  data = types(cuisine = i)
  cuisines.append(data)

def diet(special):
  """
  We divided restaurants into 2 groups: ones that have  Vegetarian menu and others that don't have.
  """
  if "Vegetarian" in special:
    diet = "Vegetarian"
      
  else:
      diet = "NotVegetarian"
  return diet

diets = []

for i in mydata.specdiet.to_list():
  data = diet(special = i)
  diets.append(data)

# Replacing the cuisine type and special diets with newly created variables.
mydata = pd.DataFrame({"names":mydata.names.to_list(),"rating":mydata.rating.to_list(),"review":mydata.review.to_list(),"cuisine":cuisines,"specdiet":diets,"min_price":mydata['min_price'],"max_price":mydata['max_price'],"url":mydata.url.to_list(),"address":mydata.address.to_list(),"latitude":mydata.latitude.to_list(),"longtitude":mydata.longtitude.to_list(),"distance":mydata.distance.to_list()})

#Droping the restaurant with extremely unreal high maxprice(Karas)
mydata = mydata.drop([57])

# So after the cleaning of the data we get our final data for analysis.
mydata

mydata.to_csv("projectdata.csv")

"""## **Analysis part**"""

#We plot a bar chart that shows top 10  restaurants by number of reviews.
sort_review = mydata.sort_values(by = "review", ascending = False)
top_review_names = sort_review.iloc[0:10]['names']
top_review = sort_review.iloc[0:10]['review']
bar_review = pd.DataFrame({"names":top_review_names,"review":top_review})
bar_review

# We have shorten the names of restaurants for bar chart
bar_review.at[18,'names']= 'Green Bean'
bar_review.at[12,'names']= 'Dargett'
bar_review.at[3,'names']= 'Indian Mehak'
bar_review.at[13,'names']= 'Anoush'
bar_review.at[31,'names']= 'Caucasus'

# Ploting
fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
ax.bar(bar_review['names'],bar_review['review'])
plt.show()

#Most expensive 10 restaurants
sort_price = mydata.sort_values(by = "max_price", ascending = False)
top_price_names= sort_price.iloc[0:10]['names']
top_price_maxp= sort_price.iloc[0:10]['max_price']
bar_price = pd.DataFrame({"names":top_price_names,"price":top_price_maxp})
bar_price

bar_price.at[34,'names']= 'Old Jrvezh'
bar_price.at[103,'names']= 'Level Eleven'
bar_price.at[108,'names']= 'Araks'
bar_price.at[130,'names']= 'TAO'
bar_price.at[89,'names']= 'Hugo'
bar_price.at[140,'names']= 'Samurai'
bar_price.at[99,'names']= 'Chinar'

# Ploting
fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
ax.bar(bar_price['names'],bar_price['price'])
plt.show()

# Droping object variables that are not needed for the regression analysis
regdata = mydata.drop(columns=['names', 'url', 'address'])

regdata.info()

# We use cuisine type and vegetarian menu as a dummy variables.
reg = pd.get_dummies(regdata,drop_first=True)

# Summary statistics of the variables 
reg.describe()

"""Regression"""

# As review numbers are relatively high we decided to use the natural logarithm of reviews.
model = smf.ols("np.log(review) ~ rating + min_price + max_price + distance + cuisine_NotArmenian + specdiet_Vegetarian", data = reg)

results = model.fit()

results.summary()
